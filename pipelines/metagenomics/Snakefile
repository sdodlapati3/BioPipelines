"""
Metagenomics Taxonomic Profiling & Functional Analysis Pipeline
================================================================

Comprehensive workflow for shotgun metagenomic sequencing analysis:
1. Quality control and preprocessing
2. Host DNA removal
3. Taxonomic profiling (Kraken2/Bracken, MetaPhlAn4)
4. Functional profiling (HUMAnN3)
5. Optional: Assembly and binning (MEGAHIT, MetaBAT2)
6. Visualization and reporting

Usage:
    snakemake --cores 16 --use-conda
"""

import os
from pathlib import Path

# Load configuration
configfile: "config.yaml"

# Samples and paths
SAMPLES = config["samples"]
RAW_DIR = config["raw_dir"]
PROCESSED_DIR = config["processed_dir"]
RESULTS_DIR = config["results_dir"]
QC_DIR = config["qc_dir"]

# Reference databases
HOST_GENOME = config["references"]["host_genome"]
HOST_INDEX = config["references"]["host_index"]
KRAKEN2_DB = config["references"]["kraken2_db"]
METAPHLAN_DB = config["references"]["metaphlan_db"]
HUMANN_DB = config["references"]["humann_db"]

# Analysis options
ASSEMBLY_ENABLED = config["params"]["assembly"]["enabled"]
BINNING_ENABLED = config["params"]["binning"]["enabled"]
HOST_REMOVAL = config["params"]["host_removal"]["enabled"]
FUNCTIONAL_ENABLED = config["params"].get("functional_profiling", {}).get("enabled", False)
KRAKEN2_ENABLED = config["params"].get("kraken2_enabled", False)

# =============================================================================
# Target Rule
# =============================================================================

rule all:
    input:
        # QC reports
        f"{RESULTS_DIR}/multiqc_report.html",
        
        # Taxonomic profiles
        expand(f"{RESULTS_DIR}/taxonomy/kraken2/{{sample}}_report.txt", sample=SAMPLES) if KRAKEN2_ENABLED else [],
        expand(f"{RESULTS_DIR}/taxonomy/bracken/{{sample}}_bracken.txt", sample=SAMPLES) if KRAKEN2_ENABLED else [],
        expand(f"{RESULTS_DIR}/taxonomy/metaphlan/{{sample}}_profile.txt", sample=SAMPLES),
        
        # Functional profiles (optional)
        expand(f"{RESULTS_DIR}/function/humann/{{sample}}_pathabundance.tsv", sample=SAMPLES) if FUNCTIONAL_ENABLED else [],
        
        # Visualizations
        expand(f"{RESULTS_DIR}/taxonomy/krona/{{sample}}_krona.html", sample=SAMPLES) if KRAKEN2_ENABLED else [],
        
        # Optional assembly
        expand(f"{RESULTS_DIR}/assembly/{{sample}}/final.contigs.fa", sample=SAMPLES) if ASSEMBLY_ENABLED else [],
        
        # Optional binning
        expand(f"{RESULTS_DIR}/bins/{{sample}}/checkm_summary.txt", sample=SAMPLES) if BINNING_ENABLED else []

# =============================================================================
# Quality Control & Preprocessing
# =============================================================================

rule fastqc_raw:
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    input:
        r1=f"{RAW_DIR}/{{sample}}_R1.fastq.gz",
        r2=f"{RAW_DIR}/{{sample}}_R2.fastq.gz"
    output:
        html1=f"{QC_DIR}/fastqc_raw/{{sample}}_R1_fastqc.html",
        html2=f"{QC_DIR}/fastqc_raw/{{sample}}_R2_fastqc.html",
        zip1=f"{QC_DIR}/fastqc_raw/{{sample}}_R1_fastqc.zip",
        zip2=f"{QC_DIR}/fastqc_raw/{{sample}}_R2_fastqc.zip"
    # Note: No conda directive - uses fastqc from base environment
    threads: 4
    log:
        f"{QC_DIR}/logs/fastqc_raw/{{sample}}.log"
    params:
        outdir=f"{QC_DIR}/fastqc_raw"
    shell:
        """
        mkdir -p {params.outdir}
        fastqc -t {threads} {input.r1} {input.r2} -o {params.outdir} 2> {log}
        """

rule trim_reads:
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    input:
        r1=f"{RAW_DIR}/{{sample}}_R1.fastq.gz",
        r2=f"{RAW_DIR}/{{sample}}_R2.fastq.gz"
    output:
        r1=f"{PROCESSED_DIR}/trimmed/{{sample}}_R1.trimmed.fastq.gz",
        r2=f"{PROCESSED_DIR}/trimmed/{{sample}}_R2.trimmed.fastq.gz",
        html=f"{QC_DIR}/fastp/{{sample}}.html",
        json=f"{QC_DIR}/fastp/{{sample}}.json"
    # Note: No conda directive - uses fastp from base environment
    threads: config["resources"]["threads"]["trim"]
    log:
        f"{QC_DIR}/logs/trim/{{sample}}.log"
    params:
        quality=config["params"]["trim"]["quality"],
        min_length=config["params"]["trim"]["min_length"]
    shell:
        """
        fastp -i {input.r1} -I {input.r2} \
              -o {output.r1} -O {output.r2} \
              --qualified_quality_phred {params.quality} \
              --length_required {params.min_length} \
              --detect_adapter_for_pe \
              --html {output.html} \
              --json {output.json} \
              --thread {threads} \
              2> {log}
        """

rule remove_host:
    input:
        r1=f"{PROCESSED_DIR}/trimmed/{{sample}}_R1.trimmed.fastq.gz",
        r2=f"{PROCESSED_DIR}/trimmed/{{sample}}_R2.trimmed.fastq.gz"
    output:
        r1=f"{PROCESSED_DIR}/host_removed/{{sample}}_R1.clean.fastq.gz",
        r2=f"{PROCESSED_DIR}/host_removed/{{sample}}_R2.clean.fastq.gz",
        stats=f"{QC_DIR}/host_removal/{{sample}}_stats.txt"
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    threads: config["resources"]["threads"]["host_removal"]
    log:
        f"{QC_DIR}/logs/host_removal/{{sample}}.log"
    params:
        index=HOST_INDEX,
        sensitivity=config["params"]["host_removal"]["sensitivity"]
    shell:
        """
        bowtie2 -x {params.index} \
                -1 {input.r1} -2 {input.r2} \
                --threads {threads} \
                --{params.sensitivity} \
                --un-conc-gz {PROCESSED_DIR}/host_removed/{wildcards.sample}_R%.clean.fastq.gz \
                -S /dev/null \
                2> {output.stats}
        
        # Log statistics
        echo "Host removal stats for {wildcards.sample}:" > {log}
        cat {output.stats} >> {log}
        """

# =============================================================================
# Taxonomic Profiling
# =============================================================================

rule kraken2_classify:
    input:
        r1=f"{PROCESSED_DIR}/host_removed/{{sample}}_R1.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R1.trimmed.fastq.gz",
        r2=f"{PROCESSED_DIR}/host_removed/{{sample}}_R2.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R2.trimmed.fastq.gz"
    output:
        report=f"{RESULTS_DIR}/taxonomy/kraken2/{{sample}}_report.txt",
        output=f"{RESULTS_DIR}/taxonomy/kraken2/{{sample}}_output.txt"
    # Note: No conda directive - uses run: block which doesn't support conda
    threads: config["resources"]["threads"]["kraken2"]
    log:
        f"{QC_DIR}/logs/kraken2/{{sample}}.log"
    params:
        db=KRAKEN2_DB,
        confidence=config["params"]["kraken2"]["confidence"]
    run:
        if not KRAKEN2_ENABLED:
            shell("touch {output.report} {output.output}")
        else:
            shell("""
            kraken2 --db {params.db} \
                    --threads {threads} \
                    --paired {input.r1} {input.r2} \
                    --report {output.report} \
                    --output {output.output} \
                    --confidence {params.confidence} \
                    --use-names \
                    2> {log}
            """)

rule bracken_abundance:
    input:
        report=f"{RESULTS_DIR}/taxonomy/kraken2/{{sample}}_report.txt"
    output:
        bracken=f"{RESULTS_DIR}/taxonomy/bracken/{{sample}}_bracken.txt",
        report=f"{RESULTS_DIR}/taxonomy/bracken/{{sample}}_bracken_report.txt"
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    log:
        f"{QC_DIR}/logs/bracken/{{sample}}.log"
    params:
        db=KRAKEN2_DB,
        read_len=config["params"]["bracken"]["read_length"],
        level=config["params"]["bracken"]["taxonomic_level"],
        threshold=config["params"]["bracken"]["threshold"]
    shell:
        """
        bracken -d {params.db} \
                -i {input.report} \
                -o {output.bracken} \
                -w {output.report} \
                -r {params.read_len} \
                -l {params.level} \
                -t {params.threshold} \
                2> {log}
        """

rule metaphlan_profile:
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    input:
        r1=f"{PROCESSED_DIR}/host_removed/{{sample}}_R1.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R1.trimmed.fastq.gz",
        r2=f"{PROCESSED_DIR}/host_removed/{{sample}}_R2.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R2.trimmed.fastq.gz"
    output:
        profile=f"{RESULTS_DIR}/taxonomy/metaphlan/{{sample}}_profile.txt",
        bowtie2out=f"{RESULTS_DIR}/taxonomy/metaphlan/{{sample}}.bowtie2.bz2"
    # Note: Uses metaphlan from base environment (installed via pip)
    threads: config["resources"]["threads"]["metaphlan"]
    log:
        f"{QC_DIR}/logs/metaphlan/{{sample}}.log"
    params:
        db=METAPHLAN_DB,
        level=config["params"]["metaphlan"]["taxonomic_level"]
    shell:
        """
        metaphlan {input.r1},{input.r2} \
                  --input_type fastq \
                  --nproc {threads} \
                  --db_dir {params.db} \
                  --mapout {output.bowtie2out} \
                  --tax_lev {params.level} \
                  -o {output.profile} \
                  2> {log}
        """

# =============================================================================
# Functional Profiling
# =============================================================================

rule humann_profile:
    input:
        r1=f"{PROCESSED_DIR}/host_removed/{{sample}}_R1.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R1.trimmed.fastq.gz",
        r2=f"{PROCESSED_DIR}/host_removed/{{sample}}_R2.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R2.trimmed.fastq.gz",
        metaphlan=f"{RESULTS_DIR}/taxonomy/metaphlan/{{sample}}_profile.txt"
    output:
        genefamilies=f"{RESULTS_DIR}/function/humann/{{sample}}_genefamilies.tsv",
        pathabundance=f"{RESULTS_DIR}/function/humann/{{sample}}_pathabundance.tsv",
        pathcoverage=f"{RESULTS_DIR}/function/humann/{{sample}}_pathcoverage.tsv"
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    threads: config["resources"]["threads"]["humann"]
    log:
        f"{QC_DIR}/logs/humann/{{sample}}.log"
    params:
        db=HUMANN_DB,
        outdir=f"{RESULTS_DIR}/function/humann/temp_{{sample}}",
        protein_db=config["params"]["humann"]["protein_db"]
    shell:
        """
        # Concatenate paired-end reads for HUMAnN
        zcat {input.r1} {input.r2} | gzip > {params.outdir}_concat.fastq.gz
        
        # Run HUMAnN3
        humann --input {params.outdir}_concat.fastq.gz \
               --output {params.outdir} \
               --threads {threads} \
               --taxonomic-profile {input.metaphlan} \
               --protein-database {params.protein_db} \
               2> {log}
        
        # Move outputs
        mv {params.outdir}/*_genefamilies.tsv {output.genefamilies}
        mv {params.outdir}/*_pathabundance.tsv {output.pathabundance}
        mv {params.outdir}/*_pathcoverage.tsv {output.pathcoverage}
        
        # Cleanup
        rm -rf {params.outdir} {params.outdir}_concat.fastq.gz
        """

# =============================================================================
# Assembly & Binning (Optional)
# =============================================================================

rule megahit_assembly:
    input:
        r1=f"{PROCESSED_DIR}/host_removed/{{sample}}_R1.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R1.trimmed.fastq.gz",
        r2=f"{PROCESSED_DIR}/host_removed/{{sample}}_R2.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R2.trimmed.fastq.gz"
    output:
        contigs=f"{RESULTS_DIR}/assembly/{{sample}}/final.contigs.fa"
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    threads: config["resources"]["threads"]["assembly"]
    log:
        f"{QC_DIR}/logs/assembly/{{sample}}.log"
    params:
        outdir=f"{RESULTS_DIR}/assembly/{{sample}}",
        min_contig=config["params"]["assembly"]["min_contig_len"]
    shell:
        """
        megahit -1 {input.r1} -2 {input.r2} \
                -o {params.outdir} \
                --min-contig-len {params.min_contig} \
                --num-cpu-threads {threads} \
                2> {log}
        """

rule metabat2_binning:
    input:
        contigs=f"{RESULTS_DIR}/assembly/{{sample}}/final.contigs.fa",
        r1=f"{PROCESSED_DIR}/host_removed/{{sample}}_R1.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R1.trimmed.fastq.gz",
        r2=f"{PROCESSED_DIR}/host_removed/{{sample}}_R2.clean.fastq.gz" if HOST_REMOVAL else f"{PROCESSED_DIR}/trimmed/{{sample}}_R2.trimmed.fastq.gz"
    output:
        depth=f"{RESULTS_DIR}/bins/{{sample}}/depth.txt",
        bins=directory(f"{RESULTS_DIR}/bins/{{sample}}/metabat_bins")
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    threads: config["resources"]["threads"]["binning"]
    log:
        f"{QC_DIR}/logs/binning/{{sample}}.log"
    params:
        min_bin=config["params"]["binning"]["min_bin_size"]
    shell:
        """
        # Map reads back to contigs
        bowtie2-build {input.contigs} {input.contigs} 2>> {log}
        bowtie2 -x {input.contigs} -1 {input.r1} -2 {input.r2} \
                --threads {threads} | samtools sort -@ {threads} \
                -o {RESULTS_DIR}/bins/{wildcards.sample}/mapped.bam 2>> {log}
        
        # Calculate depth
        jgi_summarize_bam_contig_depths --outputDepth {output.depth} \
                                        {RESULTS_DIR}/bins/{wildcards.sample}/mapped.bam
        
        # Bin with MetaBAT2
        mkdir -p {output.bins}
        metabat2 -i {input.contigs} \
                 -a {output.depth} \
                 -o {output.bins}/bin \
                 -m {params.min_bin} \
                 --numThreads {threads} \
                 2>> {log}
        """

rule checkm_quality:
    input:
        bins=f"{RESULTS_DIR}/bins/{{sample}}/metabat_bins"
    output:
        summary=f"{RESULTS_DIR}/bins/{{sample}}/checkm_summary.txt"
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    threads: config["resources"]["threads"]["binning"]
    log:
        f"{QC_DIR}/logs/checkm/{{sample}}.log"
    params:
        outdir=f"{RESULTS_DIR}/bins/{{sample}}/checkm_output"
    shell:
        """
        checkm lineage_wf -t {threads} \
                          -x fa \
                          {input.bins} \
                          {params.outdir} \
                          > {output.summary} \
                          2> {log}
        """

# =============================================================================
# Visualization
# =============================================================================

rule krona_visualization:
    input:
        report=f"{RESULTS_DIR}/taxonomy/kraken2/{{sample}}_report.txt"
    output:
        html=f"{RESULTS_DIR}/taxonomy/krona/{{sample}}_krona.html"
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    log:
        f"{QC_DIR}/logs/krona/{{sample}}.log"
    shell:
        """
        ktImportTaxonomy -t 5 -m 3 -o {output.html} {input.report} 2> {log}
        """

rule multiqc:
    container: "/home/sdodl001_odu_edu/BioPipelines/containers/images/metagenomics_1.0.0.sif"
    input:
        fastqc=expand(f"{QC_DIR}/fastqc_raw/{{sample}}_R1_fastqc.html", sample=SAMPLES),
        fastp=expand(f"{QC_DIR}/fastp/{{sample}}.json", sample=SAMPLES),
        kraken=expand(f"{RESULTS_DIR}/taxonomy/kraken2/{{sample}}_report.txt", sample=SAMPLES)
    output:
        f"{RESULTS_DIR}/multiqc_report.html"
    # Note: Uses multiqc from base environment (installed via pip)
    log:
        f"{QC_DIR}/logs/multiqc.log"
    shell:
        """
        multiqc {QC_DIR} {RESULTS_DIR}/taxonomy/kraken2 \
                -o {RESULTS_DIR} -f 2> {log}
        """
