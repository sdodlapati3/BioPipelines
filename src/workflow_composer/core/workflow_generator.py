"""
Workflow Generator
==================

Generates complete Nextflow DSL2 workflows from parsed intents and modules.

Supports:
- Pattern-based workflow generation
- Module chaining with proper data flow
- Parameter configuration
- Config file generation

Example:
    generator = WorkflowGenerator(patterns_path)
    workflow = generator.generate(intent, modules)
    workflow.save("output_dir/")
"""

import re
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, Any, List, Optional
from datetime import datetime

from .intent_parser import ParsedIntent, AnalysisType
from .module_mapper import Module, TOOL_CONTAINER_MAP
from ..llm.base import LLMAdapter, Message

logger = logging.getLogger(__name__)


@dataclass
class Workflow:
    """Represents a generated Nextflow workflow."""
    name: str
    main_nf: str
    config: str
    samplesheet_template: str = ""
    readme: str = ""
    
    output_dir: Optional[Path] = None
    modules_used: List[Module] = field(default_factory=list)
    
    def save(self, output_dir: str) -> Path:
        """
        Save workflow files to directory.
        
        Args:
            output_dir: Directory to save files
            
        Returns:
            Path to output directory
        """
        out_path = Path(output_dir)
        out_path.mkdir(parents=True, exist_ok=True)
        
        # Save main workflow
        (out_path / "main.nf").write_text(self.main_nf)
        
        # Save config
        (out_path / "nextflow.config").write_text(self.config)
        
        # Save samplesheet template
        if self.samplesheet_template:
            (out_path / "samplesheet.csv").write_text(self.samplesheet_template)
        
        # Save README
        if self.readme:
            (out_path / "README.md").write_text(self.readme)
        
        # Create modules symlink or copy structure
        modules_dir = out_path / "modules"
        modules_dir.mkdir(exist_ok=True)
        
        self.output_dir = out_path
        logger.info(f"Workflow saved to: {out_path}")
        
        return out_path


# Workflow templates for common analysis types
WORKFLOW_TEMPLATES = {
    AnalysisType.RNA_SEQ_DE: """#!/usr/bin/env nextflow
nextflow.enable.dsl = 2

/*
 * RNA-seq Differential Expression Workflow
 * Generated by BioPipelines Workflow Composer
 * Date: {date}
 */

// Import modules
{imports}

// Parameters
params.reads = "{reads_pattern}"
params.genome = "{genome}"
params.gtf = "{gtf}"
params.outdir = "results"
params.samplesheet = "samplesheet.csv"

// Main workflow
workflow {{
    // Input channels
    reads_ch = Channel
        .fromFilePairs(params.reads, checkIfExists: true)
        .map {{ sample_id, reads -> tuple([id: sample_id], reads) }}
    
    genome = file(params.genome, checkIfExists: true)
    gtf = file(params.gtf, checkIfExists: true)
    
    // QC raw reads
    FASTQC(reads_ch)
    
    // Build STAR index
    STAR_INDEX(genome, gtf)
    
    // Align reads
    STAR_ALIGN(reads_ch, STAR_INDEX.out.index, gtf)
    
    // Count features
    FEATURECOUNTS(STAR_ALIGN.out.bam, gtf)
    
    // Collect counts for DE analysis
    counts_ch = FEATURECOUNTS.out.counts.collect()
    
    // Differential expression
    DESEQ2_DIFFERENTIAL(
        counts_ch,
        file(params.samplesheet),
        "{condition_col}",
        "{reference}",
        "{treatment}"
    )
    
    // Aggregate QC
    MULTIQC(
        FASTQC.out.zip
            .mix(STAR_ALIGN.out.log)
            .mix(FEATURECOUNTS.out.summary)
            .collect()
    )
}}
""",

    AnalysisType.CHIP_SEQ: """#!/usr/bin/env nextflow
nextflow.enable.dsl = 2

/*
 * ChIP-seq Peak Calling Workflow
 * Generated by BioPipelines Workflow Composer
 * Date: {date}
 */

// Import modules
{imports}

// Parameters
params.treatment = "{treatment_pattern}"
params.control = "{control_pattern}"
params.genome = "{genome}"
params.outdir = "results"

// Main workflow
workflow {{
    // Input channels
    treatment_ch = Channel
        .fromFilePairs(params.treatment, checkIfExists: true)
        .map {{ sample_id, reads -> tuple([id: sample_id, type: 'treatment'], reads) }}
    
    control_ch = Channel
        .fromFilePairs(params.control, checkIfExists: true)
        .map {{ sample_id, reads -> tuple([id: sample_id, type: 'control'], reads) }}
    
    genome = file(params.genome, checkIfExists: true)
    
    // QC
    FASTQC(treatment_ch.mix(control_ch))
    
    // Build index
    BWA_INDEX(genome)
    
    // Align reads
    BWA_MEM(treatment_ch.mix(control_ch), BWA_INDEX.out.index)
    
    // Sort and index BAMs
    SAMTOOLS_SORT(BWA_MEM.out.bam)
    SAMTOOLS_INDEX(SAMTOOLS_SORT.out.bam)
    
    // Separate treatment and control
    treatment_bam = SAMTOOLS_SORT.out.bam
        .filter {{ meta, bam -> meta.type == 'treatment' }}
    control_bam = SAMTOOLS_SORT.out.bam
        .filter {{ meta, bam -> meta.type == 'control' }}
    
    // Peak calling
    MACS2_CALLPEAK(treatment_bam, control_bam, 'narrow', '{genome_size}')
    
    // Motif analysis
    HOMER_FINDMOTIFSGENOME(MACS2_CALLPEAK.out.peaks, genome, 200)
    
    // QC aggregation
    MULTIQC(
        FASTQC.out.zip
            .mix(MACS2_CALLPEAK.out.qc)
            .collect()
    )
}}
""",

    AnalysisType.WGS_VARIANT_CALLING: """#!/usr/bin/env nextflow
nextflow.enable.dsl = 2

/*
 * WGS Variant Calling Workflow
 * Generated by BioPipelines Workflow Composer
 * Date: {date}
 */

// Import modules
{imports}

// Parameters
params.reads = "{reads_pattern}"
params.genome = "{genome}"
params.known_sites = "{known_sites}"
params.outdir = "results"

// Main workflow
workflow {{
    // Input channels
    reads_ch = Channel
        .fromFilePairs(params.reads, checkIfExists: true)
        .map {{ sample_id, reads -> tuple([id: sample_id], reads) }}
    
    genome = file(params.genome, checkIfExists: true)
    known_sites = file(params.known_sites, checkIfExists: true)
    
    // QC
    FASTQC(reads_ch)
    
    // Build index
    BWA_INDEX(genome)
    
    // Align
    BWA_MEM(reads_ch, BWA_INDEX.out.index)
    
    // Sort
    SAMTOOLS_SORT(BWA_MEM.out.bam)
    
    // Mark duplicates
    GATK_MARKDUPLICATES(SAMTOOLS_SORT.out.bam)
    
    // Base recalibration
    GATK_BASERECALIBRATOR(GATK_MARKDUPLICATES.out.bam, genome, known_sites)
    GATK_APPLYBQSR(GATK_MARKDUPLICATES.out.bam, GATK_BASERECALIBRATOR.out.table, genome)
    
    // Variant calling
    GATK_HAPLOTYPECALLER(GATK_APPLYBQSR.out.bam, genome, Channel.empty())
    
    // Filter variants
    BCFTOOLS_FILTER(GATK_HAPLOTYPECALLER.out.vcf, "QUAL>30 && DP>10")
    
    // Stats
    BCFTOOLS_STATS(BCFTOOLS_FILTER.out.vcf)
    
    // QC aggregation
    MULTIQC(FASTQC.out.zip.mix(BCFTOOLS_STATS.out.stats).collect())
}}
""",

    AnalysisType.SCRNA_SEQ: """#!/usr/bin/env nextflow
nextflow.enable.dsl = 2

/*
 * Single-cell RNA-seq Workflow
 * Generated by BioPipelines Workflow Composer
 * Date: {date}
 */

// Import modules
{imports}

// Parameters
params.fastq_dir = "{fastq_dir}"
params.transcriptome = "{transcriptome}"
params.outdir = "results"

// Main workflow
workflow {{
    // Sample channel
    samples_ch = Channel
        .fromPath("${{params.fastq_dir}}/*", type: 'dir')
        .map {{ dir -> tuple(dir.name, dir) }}
    
    transcriptome = file(params.transcriptome, checkIfExists: true)
    
    // Cell Ranger quantification
    CELLRANGER_COUNT(samples_ch, transcriptome)
    
    // Seurat analysis
    SEURAT_QC_NORMALIZE(
        CELLRANGER_COUNT.out.filtered_matrix,
        200,   // min_features
        2500,  // max_features  
        5      // max_mito_percent
    )
    
    SEURAT_CLUSTER(
        SEURAT_QC_NORMALIZE.out.seurat_object,
        {resolution}  // clustering resolution
    )
}}
""",

    AnalysisType.METAGENOMICS_PROFILING: """#!/usr/bin/env nextflow
nextflow.enable.dsl = 2

/*
 * Metagenomics Profiling Workflow
 * Generated by BioPipelines Workflow Composer
 * Date: {date}
 */

// Import modules
{imports}

// Parameters
params.reads = "{reads_pattern}"
params.kraken2_db = "{kraken2_db}"
params.metaphlan_db = "{metaphlan_db}"
params.outdir = "results"

// Main workflow
workflow {{
    // Input channels
    reads_ch = Channel
        .fromFilePairs(params.reads, checkIfExists: true)
        .map {{ sample_id, reads -> tuple([id: sample_id], reads) }}
    
    // QC
    FASTQC(reads_ch)
    
    // Kraken2 classification
    KRAKEN2_CLASSIFY(reads_ch, file(params.kraken2_db))
    
    // Bracken abundance
    BRACKEN(KRAKEN2_CLASSIFY.out.report, file(params.kraken2_db), 150, 'S')
    
    // MetaPhlAn profiling
    METAPHLAN_PROFILE(reads_ch, file(params.metaphlan_db))
    
    // Merge profiles
    METAPHLAN_MERGE(METAPHLAN_PROFILE.out.profile.map {{ it[1] }}.collect())
    
    // QC aggregation
    MULTIQC(FASTQC.out.zip.collect())
}}
"""
}


class WorkflowGenerator:
    """
    Generates complete Nextflow workflows from intents and modules.
    """
    
    def __init__(
        self,
        patterns_path: Optional[str] = None,
        module_base_path: str = "nextflow-modules"
    ):
        """
        Initialize workflow generator.
        
        Args:
            patterns_path: Path to COMPOSITION_PATTERNS.md (optional)
            module_base_path: Base path for module imports
        """
        self.patterns_path = Path(patterns_path) if patterns_path else None
        self.module_base_path = module_base_path
    
    def generate(
        self,
        intent: ParsedIntent,
        modules: List[Module],
        llm: Optional[LLMAdapter] = None
    ) -> Workflow:
        """
        Generate a complete workflow.
        
        Args:
            intent: Parsed user intent
            modules: List of modules to include
            llm: Optional LLM for custom generation
            
        Returns:
            Generated Workflow
        """
        logger.info(f"Generating workflow for: {intent.analysis_type.value}")
        
        # Check if we have a template
        if intent.analysis_type in WORKFLOW_TEMPLATES:
            return self._generate_from_template(intent, modules)
        
        # Fall back to LLM generation
        if llm:
            return self._generate_with_llm(intent, modules, llm)
        
        # Generic generation
        return self._generate_generic(intent, modules)
    
    def _generate_from_template(
        self,
        intent: ParsedIntent,
        modules: List[Module]
    ) -> Workflow:
        """Generate workflow from template."""
        template = WORKFLOW_TEMPLATES[intent.analysis_type]
        
        # Generate imports
        imports = self._generate_imports(modules)
        
        # Fill in template parameters
        params = {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "imports": imports,
            "reads_pattern": "data/raw/*_R{1,2}.fastq.gz",
            "genome": f"data/references/{intent.genome_build}/genome.fa" if intent.genome_build else "data/references/genome.fa",
            "gtf": f"data/references/{intent.genome_build}/genes.gtf" if intent.genome_build else "data/references/genes.gtf",
            "known_sites": "data/references/dbsnp.vcf.gz",
            "condition_col": "condition",
            "reference": intent.conditions[0] if len(intent.conditions) > 0 else "control",
            "treatment": intent.conditions[1] if len(intent.conditions) > 1 else "treatment",
            "treatment_pattern": "data/chip/*_treatment_R{1,2}.fastq.gz",
            "control_pattern": "data/chip/*_input_R{1,2}.fastq.gz",
            "genome_size": "hs" if intent.organism == "human" else "mm",
            "fastq_dir": "data/scrna/fastqs",
            "transcriptome": "data/references/refdata-gex-GRCh38",
            "resolution": "0.5",
            "kraken2_db": "data/databases/kraken2_standard",
            "metaphlan_db": "data/databases/metaphlan",
        }
        
        main_nf = template.format(**params)
        
        # Generate config
        config = self._generate_config(intent, modules)
        
        # Generate samplesheet template
        samplesheet = self._generate_samplesheet(intent)
        
        # Generate README
        readme = self._generate_readme(intent, modules)
        
        return Workflow(
            name=f"{intent.analysis_type.value}_workflow",
            main_nf=main_nf,
            config=config,
            samplesheet_template=samplesheet,
            readme=readme,
            modules_used=modules
        )
    
    def _generate_imports(self, modules: List[Module]) -> str:
        """Generate import statements for modules."""
        imports = []
        
        for module in modules:
            if module.processes:
                process_list = "; ".join(module.processes)
                category = module.path.parent.name
                imports.append(
                    f"include {{ {process_list} }} from './{self.module_base_path}/{category}/{module.path.name}'"
                )
        
        return "\n".join(imports)
    
    def _generate_config(self, intent: ParsedIntent, modules: List[Module]) -> str:
        """Generate nextflow.config."""
        # Get unique containers
        containers = set()
        for module in modules:
            containers.add(module.container)
        
        container_params = "\n        ".join([
            f'{c} = "${{projectDir}}/containers/images/{c.replace(\"_\", \"-\")}_1.0.0.sif"'
            for c in sorted(containers)
        ])
        
        config = f"""/*
 * Nextflow Configuration
 * Generated by BioPipelines Workflow Composer
 */

// Parameters
params {{
    // Input/Output
    outdir = "results"
    
    // Containers
    containers {{
        {container_params}
    }}
    
    // Resources
    max_cpus = 16
    max_memory = '64.GB'
    max_time = '24.h'
}}

// Process configuration
process {{
    // Default resources
    cpus = 4
    memory = '8.GB'
    time = '4.h'
    
    // Error handling
    errorStrategy = 'retry'
    maxRetries = 2
    
    // Use containers
    container = params.containers.base
}}

// Executor configuration
profiles {{
    standard {{
        process.executor = 'local'
    }}
    
    slurm {{
        process.executor = 'slurm'
        process.queue = 'general'
        process.clusterOptions = '--account=default'
    }}
    
    singularity {{
        singularity.enabled = true
        singularity.autoMounts = true
    }}
}}

// Manifest
manifest {{
    name = '{intent.analysis_type.value}'
    author = 'BioPipelines'
    description = 'Auto-generated {intent.analysis_type.value} workflow'
    version = '1.0.0'
    nextflowVersion = '>=23.04.0'
}}
"""
        return config
    
    def _generate_samplesheet(self, intent: ParsedIntent) -> str:
        """Generate samplesheet template."""
        if intent.has_comparison:
            return """sample,fastq_1,fastq_2,condition
sample1,data/raw/sample1_R1.fastq.gz,data/raw/sample1_R2.fastq.gz,control
sample2,data/raw/sample2_R1.fastq.gz,data/raw/sample2_R2.fastq.gz,control
sample3,data/raw/sample3_R1.fastq.gz,data/raw/sample3_R2.fastq.gz,treatment
sample4,data/raw/sample4_R1.fastq.gz,data/raw/sample4_R2.fastq.gz,treatment
"""
        else:
            return """sample,fastq_1,fastq_2
sample1,data/raw/sample1_R1.fastq.gz,data/raw/sample1_R2.fastq.gz
sample2,data/raw/sample2_R1.fastq.gz,data/raw/sample2_R2.fastq.gz
"""
    
    def _generate_readme(self, intent: ParsedIntent, modules: List[Module]) -> str:
        """Generate README documentation."""
        module_list = "\n".join([f"- {m.name}: {', '.join(m.processes)}" for m in modules])
        
        return f"""# {intent.analysis_type.value.replace('_', ' ').title()} Workflow

Generated by BioPipelines Workflow Composer on {datetime.now().strftime("%Y-%m-%d")}

## Description

{intent.original_query}

## Analysis Type

{intent.analysis_type.value}

## Organism

{intent.organism or 'Not specified'} ({intent.genome_build or 'genome build not specified'})

## Modules Used

{module_list}

## Usage

```bash
# Run with local executor
nextflow run main.nf -profile singularity

# Run on SLURM cluster
nextflow run main.nf -profile slurm,singularity

# Resume failed run
nextflow run main.nf -resume
```

## Input Files

Edit `samplesheet.csv` with your sample information.

## Output

Results will be saved to `results/` directory.

## Requirements

- Nextflow >= 23.04.0
- Singularity
- BioPipelines containers
"""
    
    def _generate_generic(self, intent: ParsedIntent, modules: List[Module]) -> Workflow:
        """Generate a generic workflow structure."""
        imports = self._generate_imports(modules)
        
        # Create simple workflow skeleton
        main_nf = f"""#!/usr/bin/env nextflow
nextflow.enable.dsl = 2

/*
 * {intent.analysis_type.value.replace('_', ' ').title()} Workflow
 * Generated by BioPipelines Workflow Composer
 * Date: {datetime.now().strftime("%Y-%m-%d")}
 */

// Import modules
{imports}

// Parameters
params.reads = "data/raw/*_R{{1,2}}.fastq.gz"
params.outdir = "results"

// Main workflow
workflow {{
    // Input channel
    reads_ch = Channel
        .fromFilePairs(params.reads, checkIfExists: true)
        .map {{ sample_id, reads -> tuple([id: sample_id], reads) }}
    
    // TODO: Add workflow steps
    // This is a skeleton - customize based on your analysis needs
}}
"""
        
        config = self._generate_config(intent, modules)
        samplesheet = self._generate_samplesheet(intent)
        readme = self._generate_readme(intent, modules)
        
        return Workflow(
            name=f"{intent.analysis_type.value}_workflow",
            main_nf=main_nf,
            config=config,
            samplesheet_template=samplesheet,
            readme=readme,
            modules_used=modules
        )
    
    def _generate_with_llm(
        self,
        intent: ParsedIntent,
        modules: List[Module],
        llm: LLMAdapter
    ) -> Workflow:
        """Generate workflow using LLM for complex/custom cases."""
        imports = self._generate_imports(modules)
        
        module_info = "\n".join([
            f"- {m.name}: processes={m.processes}, container={m.container}"
            for m in modules
        ])
        
        prompt = f"""Generate a complete Nextflow DSL2 workflow for this analysis:

Analysis Type: {intent.analysis_type.value}
Original Request: {intent.original_query}
Organism: {intent.organism}
Genome: {intent.genome_build}
Data Type: {intent.data_type}
Paired-end: {intent.paired_end}
Comparison: {intent.has_comparison}
Conditions: {intent.conditions}

Available modules (already imported):
{module_info}

Import statements (use these):
{imports}

Generate a complete main.nf file with:
1. Proper parameter definitions
2. Input channel creation
3. Process calls in logical order
4. Proper channel connections
5. Output aggregation

Generate ONLY the Nextflow code."""

        messages = [
            Message.system("You are an expert Nextflow workflow developer."),
            Message.user(prompt)
        ]
        
        response = llm.chat(messages, temperature=0.1, max_tokens=4096)
        
        main_nf = response.content
        if "```" in main_nf:
            lines = main_nf.split("\n")
            in_block = False
            code_lines = []
            for line in lines:
                if line.startswith("```"):
                    in_block = not in_block
                    continue
                if in_block:
                    code_lines.append(line)
            main_nf = "\n".join(code_lines)
        
        config = self._generate_config(intent, modules)
        samplesheet = self._generate_samplesheet(intent)
        readme = self._generate_readme(intent, modules)
        
        return Workflow(
            name=f"{intent.analysis_type.value}_workflow",
            main_nf=main_nf,
            config=config,
            samplesheet_template=samplesheet,
            readme=readme,
            modules_used=modules
        )
