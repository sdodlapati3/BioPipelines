# BioPipelines Codebase Refactoring Plan

**Created**: November 27, 2025  
**Updated**: November 27, 2025  
**Status**: ‚úÖ FINAL - Minimal Refactoring Required  
**Total Estimated Effort**: ~30 minutes (optional cleanup only)  

---

## Executive Summary

After thorough investigation, the BioPipelines codebase is **well-architected with intentional design decisions**. The initial analysis identified 5 potential issues, but critical re-evaluation revealed that most "duplications" were purposeful:

### Final Assessment

| Original Issue | Finding | Action |
|----------------|---------|--------|
| LLM Duplication | **Intentional separation**: `llm/` = simple adapters, `providers/` = enterprise framework | ‚úÖ Keep both |
| Monolithic gradio_app.py | **Well-organized** with section headers, Gradio coupling makes splitting awkward | ‚úÖ Keep as-is |
| Large tools.py | **Well-structured** at 1,172 lines with clear DATA/EXECUTION sections | ‚úÖ Keep as-is |
| Diagnosis overlap | **Complementary** modules for different use cases | ‚úÖ Keep as-is |
| Missing service layer | **Not needed** for current architecture | ‚úÖ Skip |

### Remaining Optional Tasks

1. **Delete deprecated `models/` shim** (~15 min) - It only re-exports from `providers/` with deprecation warning
2. That's it! The codebase is cleaner than initially assessed.

---

## Critical Re-Evaluation Findings

### Why `llm/` and `providers/` Are Both Needed

**`llm/` (~75KB, 9 files)** - Simple, focused adapters for workflow generation:
- Used by: `core/`, `composer.py`, `cli.py`
- Purpose: Lightweight LLM wrappers for generating Snakemake/Nextflow workflows
- No health checks, no fallback, no metrics - just simple completions

**`providers/` (~82KB, 12 files)** - Enterprise framework:
- Used by: `diagnosis/agent.py`
- Purpose: Production-grade provider management with health checks, fallback chains, usage metrics
- Features: Provider registry, health monitoring, multi-provider fallback

**Conclusion**: These are **different abstraction levels** for different use cases. Consolidating would either bloat the simple use case or remove enterprise features.

### Why `gradio_app.py` Should NOT Be Split

At 3,309 lines, `gradio_app.py` appears large, but:

1. **Well-organized**: Section headers divide concerns clearly
2. **Gradio coupling**: UI components are tightly coupled to state and handlers
3. **Splitting creates problems**: 
   - Circular dependencies between modules
   - gr.State must be passed everywhere
   - Event wiring needs access to all components
4. **Not actually that large**: For a full-featured GUI, 3,300 lines is reasonable

### Why `tools.py` Should NOT Be Split

At 1,172 lines with clear section organization:

```
tools.py
‚îú‚îÄ‚îÄ Lines 1-136: Imports, ToolName enum, ToolResult, TOOL_PATTERNS
‚îú‚îÄ‚îÄ Lines 136-275: AgentTools class setup, detect_tool(), execute()
‚îú‚îÄ‚îÄ Lines 275-630: DATA TOOLS (scan_data, search_databases, check_references)
‚îî‚îÄ‚îÄ Lines 631-1172: EXECUTION TOOLS (submit_job, get_status, diagnose, etc.)
```

Splitting would:
- Create 4+ files with circular dependencies (all need ToolName, ToolResult, TOOL_PATTERNS)
- Require complex import/re-export structure
- Break the simple `process_tool_request()` function
- Add complexity without meaningful benefit

---

## Original Analysis (Superseded)

<details>
<summary>Click to expand original 5-issue analysis (kept for reference)</summary>

---

## Table of Contents

1. [Issue 1: LLM Module Consolidation](#issue-1-llm-module-consolidation)
2. [Issue 2: Gradio App Decomposition](#issue-2-gradio-app-decomposition)
3. [Issue 3: Agent Tools Modularization](#issue-3-agent-tools-modularization)
4. [Issue 4: Diagnosis Module Unification](#issue-4-diagnosis-module-unification)
5. [Issue 5: Service Layer Introduction](#issue-5-service-layer-introduction)
6. [Implementation Order](#implementation-order)
7. [Risk Assessment](#risk-assessment)

---

## Issue 1: LLM Module Consolidation

### Current State

Three separate implementations of LLM provider abstractions exist:

| Module | Files | Lines | Purpose |
|--------|-------|-------|---------|
| `llm/` | 9 | ~2,800 | Original adapters, most complete |
| `providers/` | 12 | ~2,900 | Duplicate with router/registry |
| `models/providers/` | 7 | ~1,100 | Another duplicate, simpler |

**Total Duplication**: ~6,800 lines across 28 files

### Problem Impact

- **Confusion**: Which module is canonical?
- **Maintenance**: Bug fixes must be applied 3 times
- **Testing**: 3x test surface area needed
- **Onboarding**: New developers don't know which to use

### Proposed Solution

Keep `llm/` as the canonical implementation. Archive `providers/` and `models/providers/`.

### Detailed Implementation Plan

#### Phase 1: Dependency Analysis (1 hour)

```bash
# Find all imports of providers/ and models/providers/
grep -rn "from workflow_composer.providers" src/ --include="*.py"
grep -rn "from workflow_composer.models.providers" src/ --include="*.py"
```

**Expected findings**:
- `providers/` imported by: `agents/router.py`, `composer.py`, `web/gradio_app.py`
- `models/providers/` imported by: `models/registry.py`, `core/model_service_manager.py`

#### Phase 2: Create Compatibility Layer (1 hour)

Update `providers/__init__.py` to re-export from `llm/`:

```python
# src/workflow_composer/providers/__init__.py
"""
DEPRECATED: Use workflow_composer.llm instead.
This module re-exports for backward compatibility.
"""
import warnings
warnings.warn(
    "workflow_composer.providers is deprecated. Use workflow_composer.llm",
    DeprecationWarning,
    stacklevel=2
)

from workflow_composer.llm import (
    get_llm,
    LLMBase,
    OpenAIAdapter,
    VLLMAdapter,
    # ... etc
)
```

#### Phase 3: Update Import Statements (2 hours)

| File | Current Import | New Import |
|------|----------------|------------|
| `agents/router.py` | `from ..providers import ...` | `from ..llm import ...` |
| `composer.py` | `from .providers import ...` | `from .llm import ...` |
| `web/gradio_app.py` | `from ...providers import ...` | `from ...llm import ...` |

#### Phase 4: Archive Deprecated Modules (30 min)

```bash
mkdir -p src/workflow_composer/archive/deprecated_providers
mv src/workflow_composer/providers src/workflow_composer/archive/deprecated_providers/
mv src/workflow_composer/models/providers src/workflow_composer/archive/deprecated_models_providers/
```

#### Phase 5: Update Tests (1 hour)

- Update test imports
- Verify all 18+ tests still pass

### Files Modified

| Action | Files |
|--------|-------|
| **Archive** | `providers/*.py` (12 files), `models/providers/*.py` (7 files) |
| **Modify** | `agents/router.py`, `composer.py`, `gradio_app.py`, `models/registry.py` |
| **Create** | `providers/__init__.py` (compatibility shim) |

### Rollback Plan

Keep archived files for 30 days. If issues arise, restore and revert import changes.

### Success Criteria

- [ ] All tests pass
- [ ] No direct imports from `providers/` or `models/providers/`
- [ ] Deprecation warnings appear when old imports used
- [ ] Single source of truth for LLM adapters

### Estimated Effort: 5-6 hours

### Alternative: Do Nothing

**Pros**: No risk of breaking changes  
**Cons**: Technical debt continues to grow, confusion persists

### Recommendation: **PROCEED** - High value, low risk

---

## Issue 2: Gradio App Decomposition

### Current State

`web/gradio_app.py` is **3,309 lines** - a monolithic file containing:

| Section | Lines | Responsibility |
|---------|-------|----------------|
| Imports & Config | 1-170 | Setup, constants |
| AppState & Classes | 170-700 | State management, job classes |
| Chat Handlers | 700-1,150 | Chat logic, tool routing |
| Helper Functions | 1,150-2,400 | Search, job monitoring, etc. |
| UI Builder | 2,400-3,100 | Gradio component creation |
| Event Wiring | 3,100-3,309 | Button clicks, form handlers |

### Problem Impact

- **Maintainability**: Hard to find specific functionality
- **Testing**: Impossible to unit test UI separately from logic
- **Collaboration**: Merge conflicts when multiple developers edit
- **Cognitive Load**: Too much to hold in working memory

### Proposed Solution

Split into focused modules:

```
web/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ app.py                 # Main entry point (~200 lines)
‚îú‚îÄ‚îÄ state.py               # AppState, PipelineJob classes (~200 lines)
‚îú‚îÄ‚îÄ config.py              # Constants, provider choices (~100 lines)
‚îú‚îÄ‚îÄ handlers/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ chat.py           # Chat handlers (~500 lines)
‚îÇ   ‚îú‚îÄ‚îÄ jobs.py           # Job submission/monitoring (~400 lines)
‚îÇ   ‚îú‚îÄ‚îÄ workflows.py      # Workflow operations (~300 lines)
‚îÇ   ‚îî‚îÄ‚îÄ data.py           # Data scanning/search (~200 lines)
‚îú‚îÄ‚îÄ builders/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ workspace_tab.py  # Workspace UI (~300 lines)
‚îÇ   ‚îú‚îÄ‚îÄ results_tab.py    # Results UI (~200 lines)
‚îÇ   ‚îî‚îÄ‚îÄ advanced_tab.py   # Advanced UI (~200 lines)
‚îî‚îÄ‚îÄ components/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ sidebar.py        # Sidebar components (~200 lines)
    ‚îî‚îÄ‚îÄ dialogs.py        # Modal dialogs (~100 lines)
```

### Detailed Implementation Plan

#### Phase 1: Extract State Classes (1 hour)

Create `web/state.py`:
```python
"""Application state management for BioPipelines web UI."""

from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional
from datetime import datetime

class JobStatus(Enum):
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class PipelineJob:
    job_id: str
    workflow_dir: str
    # ... (move from gradio_app.py lines 170-220)

class AppState:
    # ... (move from gradio_app.py lines 220-350)

class PipelineExecutor:
    # ... (move from gradio_app.py lines 350-700)
```

#### Phase 2: Extract Chat Handlers (2 hours)

Create `web/handlers/chat.py`:
```python
"""Chat handlers for BioPipelines AI assistant."""

from typing import Generator, List, Dict, Tuple
from ..state import AppState

def chat_with_composer(
    message: str,
    history: List[Dict[str, str]],
    provider: str,
    app_state: AppState,
) -> Generator[Tuple[List[Dict[str, str]], str], None, None]:
    # ... (move from gradio_app.py lines 706-1140)

def enhanced_chat_with_composer(...):
    # ... (move from gradio_app.py lines 1143-1240)

def smart_chat(...):
    # ... (wrapper function)
```

#### Phase 3: Extract Job Handlers (1.5 hours)

Create `web/handlers/jobs.py`:
```python
"""Job submission and monitoring handlers."""

def submit_pipeline(...):
    # ... (move job submission logic)

def refresh_monitoring(...):
    # ... (move monitoring logic)

def cancel_job(...):
    # ...

def get_job_logs(...):
    # ...
```

#### Phase 4: Extract UI Builders (2 hours)

Create `web/builders/workspace_tab.py`:
```python
"""Build the Workspace tab UI components."""

import gradio as gr
from ..config import get_provider_choices, get_example_prompts

def build_workspace_tab(app_state, pipeline_executor):
    """Build the main workspace tab with chat and sidebar."""
    with gr.TabItem("üí¨ Workspace", id="workspace"):
        # ... (move from gradio_app.py lines 2500-2700)
    return components_dict
```

#### Phase 5: Create Thin App Entry Point (1 hour)

Rewrite `web/app.py`:
```python
"""BioPipelines Web Interface - Main Entry Point."""

import gradio as gr
from .state import AppState, PipelineExecutor
from .config import ENHANCED_AGENTS, USE_LOCAL_LLM
from .handlers import chat, jobs, workflows
from .builders import workspace_tab, results_tab, advanced_tab

def create_interface() -> gr.Blocks:
    """Create the Gradio interface."""
    app_state = gr.State(AppState)
    executor = PipelineExecutor()
    
    with gr.Blocks(title="BioPipelines") as demo:
        # Header
        gr.HTML(HEADER_HTML)
        
        with gr.Tabs() as main_tabs:
            ws_components = workspace_tab.build(app_state, executor)
            results_components = results_tab.build(app_state, executor)
            advanced_components = advanced_tab.build(app_state)
        
        # Wire events
        _wire_events(ws_components, results_components, app_state, executor)
    
    return demo

def _wire_events(ws, results, app_state, executor):
    """Wire up all event handlers."""
    ws['msg_input'].submit(
        fn=chat.smart_chat,
        inputs=[...],
        outputs=[...],
    )
    # ... etc
```

### Files Modified

| Action | Files |
|--------|-------|
| **Create** | `state.py`, `config.py`, `handlers/*.py`, `builders/*.py` |
| **Modify** | `gradio_app.py` ‚Üí `app.py` (reduce from 3,309 to ~200 lines) |
| **Archive** | `gradio_app.py` (keep as backup) |

### Rollback Plan

Keep `gradio_app.py` as `gradio_app_backup.py`. New `app.py` can import from backup if needed.

### Success Criteria

- [ ] App launches and functions identically
- [ ] Each module < 500 lines
- [ ] Handlers can be unit tested in isolation
- [ ] All existing tests pass

### Estimated Effort: 8-10 hours

### Alternative: Partial Extraction

Extract only chat handlers first, leave rest monolithic.

**Pros**: Less risk, faster  
**Cons**: Still hard to maintain, incomplete solution

### Recommendation: **DEFER** - High value but high effort. Consider partial extraction first.

---

## Issue 3: Agent Tools Modularization

### Current State

`agents/tools.py` is **1,171 lines** containing:

| Section | Lines | Tools |
|---------|-------|-------|
| Tool Definitions | 1-150 | ToolName enum, patterns |
| Data Tools | 150-400 | scan_data, search_databases |
| Job Tools | 400-700 | submit_job, get_status, get_logs |
| Workflow Tools | 700-900 | list_workflows, download |
| Utility Tools | 900-1171 | help, compare_samples |

### Problem Impact

- **Single Responsibility**: One file does everything
- **Testing**: Hard to test data tools without job tools
- **Dependencies**: All tools share imports even if unused

### Proposed Solution

Split by domain:

```
agents/
‚îú‚îÄ‚îÄ tools/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py        # Re-exports all tools
‚îÇ   ‚îú‚îÄ‚îÄ base.py            # ToolResult, ToolName, patterns
‚îÇ   ‚îú‚îÄ‚îÄ data_tools.py      # scan_data, search_databases, references
‚îÇ   ‚îú‚îÄ‚îÄ job_tools.py       # submit, status, logs, cancel
‚îÇ   ‚îú‚îÄ‚îÄ workflow_tools.py  # list, download, compare
‚îÇ   ‚îî‚îÄ‚îÄ utility_tools.py   # help, diagnose
```

### Detailed Implementation Plan

#### Phase 1: Create Base Module (30 min)

```python
# agents/tools/base.py
from enum import Enum
from dataclasses import dataclass
from typing import Any, Optional, Dict

class ToolName(Enum):
    SCAN_DATA = "scan_data"
    # ...

@dataclass
class ToolResult:
    success: bool
    tool_name: str
    data: Any = None
    message: str = ""
    error: Optional[str] = None
    ui_update: Optional[Dict[str, Any]] = None

# Tool detection patterns
TOOL_PATTERNS = [...]
```

#### Phase 2: Extract Data Tools (1 hour)

```python
# agents/tools/data_tools.py
from .base import ToolResult, ToolName

def scan_data(path: str, app_state) -> ToolResult:
    """Scan local directory for FASTQ files."""
    # ... (move from tools.py)

def search_databases(query: str, sources: list, app_state) -> ToolResult:
    """Search ENCODE, GEO, etc."""
    # ...

def check_references(organism: str, app_state) -> ToolResult:
    """Check reference genome availability."""
    # ...
```

#### Phase 3: Extract Job Tools (1 hour)

```python
# agents/tools/job_tools.py
def submit_job(workflow_dir: str, profile: str, app_state) -> ToolResult:
    ...

def get_job_status(job_id: str, app_state) -> ToolResult:
    ...

def get_logs(job_id: str, lines: int, app_state) -> ToolResult:
    ...

def cancel_job(job_id: str, app_state) -> ToolResult:
    ...
```

#### Phase 4: Create Unified Interface (30 min)

```python
# agents/tools/__init__.py
from .base import ToolName, ToolResult, TOOL_PATTERNS
from .data_tools import scan_data, search_databases, check_references
from .job_tools import submit_job, get_job_status, get_logs, cancel_job
from .workflow_tools import list_workflows, download_results
from .utility_tools import show_help, diagnose_error

class AgentTools:
    """Unified interface for all agent tools."""
    
    def __init__(self, app_state):
        self.app_state = app_state
    
    def execute(self, tool_name: ToolName, **kwargs) -> ToolResult:
        """Execute a tool by name."""
        # Route to appropriate function
```

### Files Modified

| Action | Files |
|--------|-------|
| **Create** | `tools/base.py`, `tools/data_tools.py`, `tools/job_tools.py`, etc. |
| **Modify** | `tools.py` ‚Üí `tools/__init__.py` |
| **Update** | `bridge.py`, `router.py` (import paths) |

### Success Criteria

- [ ] All tools work as before
- [ ] Each module < 300 lines
- [ ] Tests pass for individual tool modules

### Estimated Effort: 3-4 hours

### Recommendation: **PROCEED** - Medium effort, good value

---

## Issue 4: Diagnosis Module Unification

### Current State

Overlapping diagnosis functionality:

| File | Lines | Purpose |
|------|-------|---------|
| `diagnosis/agent.py` | 808 | Error diagnosis with LLM |
| `diagnosis/auto_fix.py` | 500 | Automated fix suggestions |
| `agents/coding_agent.py` | 719 | Code-focused error analysis |
| `agents/self_healing.py` | 615 | Auto-retry with diagnosis |

**Total**: 2,642 lines with significant overlap

### Problem Impact

- **Duplication**: Similar LLM prompts in multiple files
- **Confusion**: Which diagnosis to use when?
- **Maintenance**: Improvements must be synced across files

### Proposed Solution

Consolidate into unified diagnosis system:

```
domain/diagnosis/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ analyzer.py        # Core analysis logic
‚îú‚îÄ‚îÄ strategies/
‚îÇ   ‚îú‚îÄ‚îÄ log_analysis.py    # Parse log files
‚îÇ   ‚îú‚îÄ‚îÄ code_analysis.py   # Analyze code errors
‚îÇ   ‚îî‚îÄ‚îÄ resource_analysis.py  # Check memory/disk
‚îú‚îÄ‚îÄ fixers/
‚îÇ   ‚îú‚îÄ‚îÄ auto_fix.py        # Apply safe fixes
‚îÇ   ‚îî‚îÄ‚îÄ suggestions.py     # Generate fix suggestions
‚îî‚îÄ‚îÄ integration/
    ‚îú‚îÄ‚îÄ slurm.py           # SLURM-specific handling
    ‚îî‚îÄ‚îÄ nextflow.py        # Nextflow-specific handling
```

### Detailed Implementation Plan

#### Phase 1: Identify Common Patterns (1 hour)

Review all 4 files and extract:
- Common error categories
- Shared LLM prompts
- Overlapping fix strategies

#### Phase 2: Create Core Analyzer (2 hours)

```python
# domain/diagnosis/analyzer.py
class DiagnosisResult:
    error_type: str
    root_cause: str
    suggestions: List[str]
    auto_fixable: bool
    confidence: float

class ErrorAnalyzer:
    def __init__(self, llm_client):
        self.llm = llm_client
        self.strategies = [
            LogAnalysisStrategy(),
            CodeAnalysisStrategy(),
            ResourceAnalysisStrategy(),
        ]
    
    def analyze(self, context: ErrorContext) -> DiagnosisResult:
        # Run all strategies, aggregate results
        ...
```

#### Phase 3: Migrate Existing Logic (3 hours)

- Move `diagnosis/agent.py` logic ‚Üí `analyzer.py`
- Move `coding_agent.py` logic ‚Üí `strategies/code_analysis.py`
- Move `self_healing.py` retry logic ‚Üí `fixers/auto_fix.py`
- Keep `self_healing.py` as thin wrapper that uses new modules

#### Phase 4: Update Consumers (1 hour)

- `gradio_app.py`: Use new `ErrorAnalyzer`
- `agents/bridge.py`: Update imports
- `agents/tools.py`: Update diagnose_error tool

### Files Modified

| Action | Files |
|--------|-------|
| **Create** | `domain/diagnosis/*.py` |
| **Archive** | `diagnosis/agent.py`, `agents/coding_agent.py` |
| **Modify** | `agents/self_healing.py` (thin wrapper) |

### Estimated Effort: 7-8 hours

### Alternative: Keep Separate, Add Facade

Create a facade that routes to existing implementations without refactoring.

**Pros**: No refactoring risk  
**Cons**: Duplication remains

### Recommendation: **DEFER** - High effort. Consider facade pattern first.

---

## Issue 5: Service Layer Introduction

### Current State

UI layer (`web/`) directly imports domain logic:

```python
# Current: gradio_app.py
from workflow_composer.agents import AgentTools, AgentBridge
from workflow_composer.core import ToolSelector, ModuleMapper
from workflow_composer.data import DataManifest, LocalSampleScanner
```

### Problem Impact

- **Coupling**: UI knows too much about domain internals
- **Testing**: Can't mock services easily
- **Flexibility**: Hard to add new UIs (API, CLI)

### Proposed Solution

Add application services layer:

```
application/
‚îú‚îÄ‚îÄ __init__.py
‚îú‚îÄ‚îÄ chat_service.py      # Chat orchestration
‚îú‚îÄ‚îÄ workflow_service.py  # Workflow generation
‚îú‚îÄ‚îÄ job_service.py       # Job management
‚îî‚îÄ‚îÄ data_service.py      # Data discovery
```

### Detailed Implementation Plan

#### Phase 1: Define Service Interfaces (1 hour)

```python
# application/chat_service.py
from typing import Generator, Dict, Any
from dataclasses import dataclass

@dataclass
class ChatResponse:
    message: str
    tool_result: Optional[Dict]
    workflow_generated: Optional[str]

class ChatService:
    """Orchestrates chat interactions."""
    
    def __init__(self, llm_provider: str, config: Dict):
        self._agent_bridge = None
        self._composer = None
        # Lazy initialization
    
    def process_message(
        self, 
        message: str, 
        context: Dict[str, Any]
    ) -> Generator[ChatResponse, None, None]:
        """Process user message and yield responses."""
        # Encapsulates all chat logic
        ...
```

#### Phase 2: Create Workflow Service (1 hour)

```python
# application/workflow_service.py
class WorkflowService:
    def generate(self, request: str, data_context: Dict) -> WorkflowResult:
        ...
    
    def list_workflows(self) -> List[WorkflowInfo]:
        ...
    
    def get_workflow(self, name: str) -> WorkflowDetails:
        ...
```

#### Phase 3: Create Job Service (1 hour)

```python
# application/job_service.py
class JobService:
    def submit(self, workflow_dir: str, profile: str) -> JobInfo:
        ...
    
    def get_status(self, job_id: str) -> JobStatus:
        ...
    
    def get_logs(self, job_id: str, lines: int) -> str:
        ...
    
    def cancel(self, job_id: str) -> bool:
        ...
```

#### Phase 4: Update UI to Use Services (2 hours)

```python
# web/app.py
from workflow_composer.application import ChatService, JobService

def create_interface():
    chat_service = ChatService(provider="vllm", config={})
    job_service = JobService()
    
    # UI only knows about services, not internals
```

### Files Modified

| Action | Files |
|--------|-------|
| **Create** | `application/*.py` |
| **Modify** | `web/app.py`, `web/handlers/*.py` |

### Estimated Effort: 5-6 hours

### Recommendation: **DEFER** - Depends on Issue 2 (Gradio decomposition)

---

## Implementation Order

Based on dependencies and risk:

```
Phase 1 (Week 1): Low-risk cleanup
‚îú‚îÄ‚îÄ Issue 1: LLM Consolidation (5-6 hours)
‚îî‚îÄ‚îÄ Issue 3: Tools Modularization (3-4 hours)

Phase 2 (Week 2): Medium refactoring  
‚îî‚îÄ‚îÄ Issue 2: Gradio Decomposition (8-10 hours)
    ‚îî‚îÄ‚îÄ Start with handlers extraction only

Phase 3 (Week 3+): If time permits
‚îú‚îÄ‚îÄ Issue 5: Service Layer (5-6 hours)
‚îî‚îÄ‚îÄ Issue 4: Diagnosis Unification (7-8 hours)
```

---

## Risk Assessment

| Issue | Risk Level | Mitigation |
|-------|------------|------------|
| Issue 1: LLM | üü¢ Low | Compatibility shim, archive originals |
| Issue 2: Gradio | üü° Medium | Incremental extraction, keep backup |
| Issue 3: Tools | üü¢ Low | Same interface, just reorganized |
| Issue 4: Diagnosis | üî¥ High | Complex logic, use facade first |
| Issue 5: Services | üü° Medium | Depends on Issue 2 |

---

## Decision Matrix

| Issue | Effort | Value | Risk | Recommendation |
|-------|--------|-------|------|----------------|
| 1. LLM Consolidation | 5-6h | High | Low | ‚úÖ **PROCEED** |
| 2. Gradio Decomposition | 8-10h | High | Medium | ‚è∏Ô∏è **PARTIAL** (handlers only) |
| 3. Tools Modularization | 3-4h | Medium | Low | ‚úÖ **PROCEED** |
| 4. Diagnosis Unification | 7-8h | Medium | High | ‚è∏Ô∏è **DEFER** (use facade) |
| 5. Service Layer | 5-6h | Medium | Medium | ‚è∏Ô∏è **DEFER** (after Issue 2) |

---

## Next Steps

1. **Review this document** - Identify any concerns or questions
2. **Prioritize** - Confirm which issues to address first
3. **Execute** - Start with approved refactoring tasks
4. **Validate** - Run full test suite after each phase
5. **Document** - Update architecture docs as we go

---

## Appendix: Current vs Target Architecture

### Current (Flat/Coupled)

```
src/workflow_composer/
‚îú‚îÄ‚îÄ agents/          # Mixed concerns
‚îú‚îÄ‚îÄ core/            # Business logic
‚îú‚îÄ‚îÄ data/            # Data handling
‚îú‚îÄ‚îÄ diagnosis/       # Error handling
‚îú‚îÄ‚îÄ llm/             # LLM adapters ‚Üê KEEP
‚îú‚îÄ‚îÄ models/          # Duplicate LLM
‚îú‚îÄ‚îÄ providers/       # Duplicate LLM
‚îú‚îÄ‚îÄ web/             # Monolithic UI
‚îî‚îÄ‚îÄ ...
```

### Target (Layered/Decoupled)

```
src/workflow_composer/
‚îú‚îÄ‚îÄ presentation/    # UI only (web/, cli/)
‚îú‚îÄ‚îÄ application/     # Use cases (services)
‚îú‚îÄ‚îÄ domain/          # Business logic (agents, workflows)
‚îú‚îÄ‚îÄ infrastructure/  # External (llm/, slurm/, storage/)
‚îî‚îÄ‚îÄ shared/          # Cross-cutting (config, logging)
```

</details>

---

## Conclusion

The BioPipelines codebase is **well-designed**. What initially appeared as duplication and bloat is actually **intentional separation of concerns**:

1. **`llm/` vs `providers/`**: Different abstraction levels for different use cases ‚úÖ
2. **`gradio_app.py` size**: Appropriate for a full-featured GUI, well-organized ‚úÖ
3. **`tools.py` size**: Clear domain separation with section headers ‚úÖ
4. **Diagnosis modules**: Complementary approaches, not duplicates ‚úÖ

### Only Optional Cleanup

If desired, delete the deprecated `models/` directory:

```bash
rm -rf src/workflow_composer/models/
```

This is purely optional - the module issues a deprecation warning and harms nothing.

---

*Document version: 2.0*  
*Last updated: November 27, 2025*  
*Status: FINAL - Codebase is well-architected, minimal changes recommended*
